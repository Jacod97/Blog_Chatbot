from langchain.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.schema.messages import HumanMessage
from langchain.schema.output_parser import StrOutputParser
from operator import itemgetter
from dotenv import load_dotenv

load_dotenv("../.env")


class ChatBot:
    def __init__(self, retriever, session_id="default"):
        # retriever ÎûòÌïë
        self.retriever = retriever

        # LLM
        self.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7)

        # ÌîÑÎ°¨ÌîÑÌä∏ Î°úÎìú
        self.prompt = self._set_prompt()

        # ÎåÄÌôî Í∏∞Î°ù Ï†ÄÏû•ÏÜå
        self.history_store = {}
        self.session_id = session_id

        # Ï≤¥Ïù∏ ÏÉùÏÑ±
        self.chain = self._make_chain()
        self.history_chain = self._wrap_with_history()

    # ==============================
    # Prompt Î°úÎî©
    # ==============================
    def _set_prompt(self):
        prompt_path = "../prompts/chatbot.prompt"
        with open(prompt_path, "r", encoding="utf-8") as f:
            template = f.read()
        return ChatPromptTemplate.from_template(template)

    # ==============================
    # Chain ÏÉùÏÑ±
    # ==============================
    def _make_chain(self):
        def build_context(x):
            # retrieverÏóêÏÑú Î¨∏ÏÑú Í∞ÄÏ†∏Ïò§Í∏∞
            retrieved_docs = self.retriever.get_relevant_documents(x["question"].content)
            return "\n".join(d.page_content for d in retrieved_docs)

        chain = (
            {
                "context": build_context,                # üìå retriever Í≤∞Í≥ºÎßå
                "question": itemgetter("question"),      # ÏßàÎ¨∏
                "chat_history": itemgetter("chat_history")  # ÎåÄÌôî Í∏∞Î°ù (Í∑∏ÎåÄÎ°ú Ï†ÑÎã¨)
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )
        return chain


    # ==============================
    # ÏÑ∏ÏÖòÎ≥Ñ ÎåÄÌôî Í∏∞Î°ù
    # ==============================
    def _get_session_history(self, session_id: str) -> BaseChatMessageHistory:
        if session_id not in self.history_store:
            self.history_store[session_id] = ChatMessageHistory()
        return self.history_store[session_id]

    def _wrap_with_history(self):
        return RunnableWithMessageHistory(
            self.chain,
            self._get_session_history,
            input_messages_key="question",
            history_messages_key="chat_history",
        )

    # ==============================
    # ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏ Ï≤òÎ¶¨
    # ==============================
    def ask(self, question: str, session_id=None):
        sid = session_id or self.session_id
        response = self.history_chain.invoke(
            {"question": HumanMessage(content=question)},
            config={"configurable": {"session_id": sid}},
        )
        return response
